# -*- coding: utf-8 -*-
"""Final Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bsiXJTJyiYjn0QeeqE3efi7lZPW9xzq4
"""

import numpy as np       #importing all the necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import torch 
import torch.nn as nn
import torch.nn.functional as nnf
import torch.utils.data 
from sklearn.metrics import accuracy_score
from sklearn import preprocessing
import seaborn as sns
import pickle

# from google.colab import drive
# drive.mount('/content/drive')

# p1 ="/content/drive/MyDrive/Crop_recommendation.csv"  #path of dataset(colab)

data = pd.read_csv('Dataset.csv')  #reading datset
data

sns.pairplot(data, hue="label")

"""Splitting the dataset into 80%, 20% as training data,and testing data respectively."""

X = data.drop('label', axis=1)  #slicing dataset into features and labels
y = data['label']

data_np = data.to_numpy()  #converting datasets from csv to numpy arrays for future convenience

from sklearn.model_selection import train_test_split   #splitting dataset into training and cross-validation datsets
x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=57, stratify=y)

train_size = int(0.8 * len(data_np))   #size of datasets
test_size = len(data_np) - train_size

num_crops = np.unique(data_np[:,-1])  #distinct labels 
print(num_crops, num_crops.shape)

def ohc(data,num) :   #one hot encoding of the datsets
  y = data[:]
  for i in range(num.shape[0]):
    for j in range(data.shape[0]):
      if data[j] == num[i]:
        y[j] = i
  IM = np.identity(num.shape[0])
  list = []
  for i in range(y.shape[0]):
    a = y[i]
    list.append(IM[a,:])
  y_net = np.array(list)
  return y_net

y1 = ohc(y_train.to_numpy(),num_crops)    #appling one hot encoding
pd.DataFrame(y1)
y2 = ohc(y_val.to_numpy(),num_crops)
pd.DataFrame(y2)

scaler = preprocessing.StandardScaler().fit(x_train.to_numpy())   #standardization of features
X_scaled = scaler.transform(x_train.to_numpy())
pd.DataFrame(X_scaled)
Xt_scaled = scaler.transform(x_val.to_numpy())

x = torch.from_numpy(X_scaled)   #conversion of arrays to tensors
xt = torch.from_numpy(Xt_scaled)

y = torch.from_numpy(y1)
yt = torch.from_numpy(y2)

def code(X,y,input_size, hidden_sizes, output_size, lr, num_epochs): 
  model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),   #forward-pass
                      nn.ReLU(),
                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),
                      nn.ReLU(),
                      nn.Linear(hidden_sizes[1], output_size),
                      nn.Softmax(dim=1))
  print(model)

  loss_fn = nn.BCELoss()         #cost function
  optim = torch.optim.SGD(model.parameters(), lr)      #optimization through stochastic gradient descent
  
  loss_values = []
  for epoch in range(num_epochs):
        optim.zero_grad()            #for cleaning out previous gradients from memory
       
        # forward + backward + optimize
        pred = model(X.float()).reshape(y.shape)
        loss = loss_fn(pred.float(), y.float())
        loss_values.append(loss.item())
        loss.backward()      #backprop
        optim.step()

  plt.plot(loss_values)
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.title("Learning rate %f"%(lr))
  plt.show() 


  return model

model = code(x,y,x_train.shape[1], [100,100], y.shape[1], 2, 300 )  #cross-validating the model

output = (model(xt.float())).reshape(yt.shape)    #predicting and checking the accuracy
final = output.detach().numpy()
Y_Pred = (np.argmax(final,axis=1).reshape(y_val.shape)).reshape(y_val.shape)
diff=(Y_Pred - y_val)
dif = diff.tolist()
corr = dif.count(0)
acc = (corr/y_val.shape[0])*100
acc

pickle.dump(output, open('ml.pkl', 'wb'))

